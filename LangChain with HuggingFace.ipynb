{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "920c1dc6",
   "metadata": {},
   "source": [
    "# What is GPT?\n",
    "\n",
    "**Generative pre-trained transformer (GPT)** is an initial language model introduced in 2018 by OpenAI to demonstrate the power of performing natural language processing (NLP) tasks using the transformer architecture. The advancements in the GPT model show the potential of pretraining a large language model on diverse datasets to generate coherent and context-aware text.\n",
    "\n",
    "The introduction of GPT-3 in 2020 marked a turning point because the model exhibits remarkable performance for tasks such as language translation, question answering, and creative writing to help people speed up their daily tasks.\n",
    "\n",
    "GPT-3 provides two possibilities to interact with the OpenAI LLMs. A user can visit the browser and use a conversational style to perform tasks, or alternately, a developer can utilize the APIs provided by OpenAI to integrate the GPT model into an LLM-powered application.\n",
    "\n",
    "Models like GPT-3 and GPT-3.5 lack the ability to interact and extract information from the external data resource. This gives rise to the following limitations:\n",
    "\n",
    "- **Limited knowledge**: All GPT models provide responses based on the pretrained data, and as of the time this course was written, ChatGPT’s knowledge is based on preexisting data up to January 2022. Therefore, it’s unaware of any events or updates after this cut-off date. For example, if we enquire about a recent sports event, like the result of the most recent world championship, the GPT models would not be able to answer.\n",
    "\n",
    "- **Access to private repositories**: The GPT models don’t have access to private data in a repository like Google Drive or any external websites. This limits the GPT models’ accessibility to information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e226144",
   "metadata": {},
   "source": [
    "# LangChain\n",
    "open-source framework in Python\n",
    "\n",
    "facilitates the integration of LLMs to develop LLM-powered applications with external data sources and services.\n",
    "\n",
    "It facilitates real-time, context-aware NLP tasks without requiring developers to build everything from scratch.\n",
    "## Key Features:\n",
    "**Prompt Engineering:** LangChain offers a toolkit to formalize the prompt engineering process, enabling the structured formulation of text prompts that serve as inputs to LLMs. \n",
    "Used for common operations, such as summarization, questions answering, etc.\n",
    "\n",
    "A prompt is a set of instructions used to guide an LLM in generating a desired, contextually relevant response. Precision and clarity in prompts are crucial for influencing the model's output.\n",
    "\n",
    "**Chains:** It is used to  interact with LangChain. It provides a mechanism to execute a sequence of calls to LLMs and tools through prompt templates. The tools refer to the functionalities that allow the LLMs to interact with the world, e.g., through an API call. This sequence of calls allows developers to harness the power of language models and efficiently integrate them into their applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31adf01c",
   "metadata": {},
   "source": [
    "### Getting started with HuggingFace API\n",
    "\n",
    "Let's go through the following steps to fetch HuggingFace’s API key:\n",
    "\n",
    "1. We'll start by first visiting [HuggingFace Website](https://huggingface.co/) and make and account to fetch API keys.\n",
    "\n",
    "2. Now, go to this [link](https://huggingface.co/settings/tokens) to create access tokens for API access. \n",
    "\n",
    "3. One the above page, click on `New Token` and give a name and then change the access at the bottom to read.\n",
    "\n",
    "4. Remember to copy the key because you won’t be able to view this key again once you click the \"Done\" button."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015e4574",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip -q install openai langchain huggingface_hub\n",
    "#from langchain.llms import HuggingFaceHub (deprecated)\n",
    "\n",
    "!pip install langchain_huggingface\n",
    "from langchain_huggingface import HuggingFaceEndpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3212d5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['HUGGINGFACEHUB_API_TOKEN'] = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350892c1",
   "metadata": {},
   "source": [
    "LangChain is an incredible platform that allows developers to use language models in diverse applications. It lets developers create customizable chains to fine-tune the language models according to the needs. This adaptability makes LangChain an ideal solution for a wide range of language-based tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d6ec81",
   "metadata": {},
   "source": [
    "### Models and Prompts\n",
    "\n",
    "LangChain provides a framework for developers to swiftly create LLM-powered applications. Using LangChains, developers can streamline the development process by providing several language models.\n",
    "\n",
    "A model in the context of LangChain is any language model that is used to generate a series of words trained on a probabilistic model in a natural language. Let’s look at the two types of models LangChain offers for developing applications.\n",
    "\n",
    "1. LLMs\n",
    "2. Chat Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a46630",
   "metadata": {},
   "source": [
    "### LLMs\n",
    "\n",
    "LLMs are the core part of LangChain that takes a string as an input and returns a string at the output. LangChain provides a standard interface with several LLMs, such as OpenAI, Cohere, Hugging Face, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908e67a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiating the HuggingFace LLM with API key\n",
    "llm = HuggingFaceEndpoint(repo_id=\"meta-llama/Meta-Llama-3-70B-Instruct\")\n",
    "\n",
    "# Query the model\n",
    "response = llm.invoke(\"What is the tallest building in the world?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b544a95",
   "metadata": {},
   "source": [
    "### Chat Models\n",
    "\n",
    "LangChain provides built-in integration with chat models provided by OpenAI, Cohere, Hugging Face, etc. Unlike traditional “text in, text out” interfaces, chat models adopt a distinct approach, utilizing a structure where inputs and outputs are framed as chat messages. This design facilitates a more interactive and dynamic user interaction with the language model.\n",
    "\n",
    "**The chat model in LangChain allows to communicate using different types of messages**:\n",
    "\n",
    "- **AIMessage** shows messages that represent the AI responses.\n",
    "\n",
    "- **SystemMessage** sets the objectives that the model needs to follow.\n",
    "\n",
    "- **HumanMessage** represents the query sent by the human to the AI.\n",
    "\n",
    "- **FunctionMessage** passes the function message back to the model after its execution.\n",
    "\n",
    "- **ChatMessage** enables the arbitrary setting of a role in the chat.\n",
    "\n",
    "Let's look at an example of how we can utilize the `SystemMessage` to define the objective for the AI in a chat between the AI and human:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea8972d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage,SystemMessage\n",
    "from langchain_huggingface import ChatHuggingFace\n",
    "\n",
    "chat_model = ChatHuggingFace(llm=llm)\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You're a helpful assistant\"),\n",
    "    HumanMessage(\n",
    "        content=\"What happens when an unstoppable force meets an immovable object?\"\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89285577",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = chat_model.invoke(messages)\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32f99e0",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33abd8ac",
   "metadata": {},
   "source": [
    "### Prompts\n",
    "\n",
    "A **prompt** is a query that stores the style and format of an input to a model that answers the query accordingly. Prompts play an important role in the interaction with language models. A careful crafting of prompts is essential to get the desired and effective response from the model. To ease the prompting tasks, LangChain provides prompt templates.\n",
    "\n",
    "**Prompt templates** serve as predefined recipes for crafting prompts tailored for language models and are reusable. A prompt template may include the following:\n",
    "\n",
    "- `Instructions`: This provides specific guidelines that instruct the language model on how to generate responses to queries.\n",
    "- `Few-shot examples`: This provides examples of input-output pairs that help the language model understand the context for the given prompt.\n",
    "- `User input`: This corresponds to the user’s query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76fee01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the PromptTemplate to create a template for a string prompt\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Initiating the chat model with API key\n",
    "llm = HuggingFaceEndpoint(repo_id=\"meta-llama/Meta-Llama-3-70B-Instruct\")\n",
    "\n",
    "#Define the email_template with variables that need to be filled dynamically.\n",
    "email_template = PromptTemplate.from_template(\n",
    "    \"Create an invitation email to the recipinet that is {recipient_name} \\\n",
    " for an event that is {event_type} in a language that is {language} \\\n",
    " Mention the event location that is {event_location} \\\n",
    " and event date that is {event_date}. \\\n",
    " Also write few sentences about the event description that is {event_description} \\\n",
    " in style that is {style} \"\n",
    ")\n",
    "\n",
    "# Create a prompt message from the email_template\n",
    "message = email_template.format(\n",
    "    style = \"enthusiastic tone\",\n",
    "    language = \"American english\",\n",
    "    recipient_name=\"John\",\n",
    "    event_type=\"product launch\",\n",
    "    event_date=\"January 15, 2024\",\n",
    "    event_location=\"Hotel Taj Mahal Palace\",\n",
    "    event_description=\"an exciting unveiling of our latest innovations\"\n",
    "    )\n",
    "\n",
    "response = llm.invoke(message)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d218b656",
   "metadata": {},
   "source": [
    "#### Few-shot prompt templates\n",
    "\n",
    "Few-shot is a very powerful technique to guide the response of the LLM. The idea behind few-shot prompting is to provide examples to guide the model to understand and adapt to specific tasks. Using the examples, the LLM learns to apply the knowledge to similar scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73044c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing LangChain modules\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "# Initialize our chat model by setting the temperature to 0.0. The temperature of a model basically \n",
    "# sets the randomness of responses generated by the model. The value of this variable ranges from \n",
    "# 0 to 2. If we're looking for more consistent responses, then we should set it to 0.0\n",
    "chat = ChatHuggingFace(llm=llm)\n",
    "\n",
    "examples = [\n",
    "  {\n",
    "    \"review\": \"I absolutely love this product! It exceeded my expectations.\",\n",
    "    \"sentiment\": \"Positive\"\n",
    "  },\n",
    "  {\n",
    "    \"review\": \"I'm really disappointed with the quality of this item. It didn't meet my needs.\",\n",
    "    \"sentiment\": \"Negative\"\n",
    "  },\n",
    "  {\n",
    "    \"review\": \"The product is okay, but there's room for improvement.\",\n",
    "    \"sentiment\": \"Neutral\"\n",
    "  }\n",
    "]\n",
    "\n",
    "\n",
    "# Format the example_prompt as a PromptTemplate object that converts the dataset in examples \n",
    "# into a string. Here we provide review and sentiment as input variables and specify the \n",
    "# template structure \"Review: {review}\\n{sentiment}\" to indicate how the string should be \n",
    "# formatted in the database.\n",
    "example_prompt = PromptTemplate(\n",
    "                        input_variables=[\"review\", \"sentiment\"], \n",
    "                        template=\"Review: {review}\\n{sentiment}\")\n",
    "\n",
    "prompt = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    suffix=\"Review: {input}\",\n",
    "    input_variables=[\"input\"]\n",
    ")\n",
    "\n",
    "message = prompt.format(input=\"The machine worked okay without much trouble.\")\n",
    "\n",
    "response = chat.invoke(message)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5e70d4",
   "metadata": {},
   "source": [
    "### Chains"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63414aea",
   "metadata": {},
   "source": [
    "A chain contributes to a sequence of decision-based LLM calls connected by inputs and outputs. It can handle multiple elements of an application using prompt templates, which makes the streamlined process smooth and enhances the application’s capabilities.\n",
    "\n",
    "**There are three different types of chains**:\n",
    "\n",
    "1. LLM chain\n",
    "2. Sequential chain\n",
    "3. Router chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4c17af",
   "metadata": {},
   "source": [
    "#### 1. LLM Chain\n",
    "\n",
    "An LLM chain is the simplest form of chain in LangChain. It involves input from the user, a prompt template, a language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10547954",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"book\"],\n",
    "    template=\"Name the author of the book {book}?\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4696fb28",
   "metadata": {},
   "source": [
    "The `input_variables` parameter is set to the book name, and the `template` is defined to instruct the model to name the book’s author in the prompt template. This refines the query before passing it to the model since we have separated the input and expected output variables in the prompt template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a91c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = LLMChain(llm=llm, \n",
    "                prompt=prompt_template, \n",
    "                verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70d5a55",
   "metadata": {},
   "source": [
    "Then, the prompt template is passed into the LLM chain, where the model answers the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47292cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the modules\n",
    "#from langchain.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# defining the LLM model (to be used with OpenAI)\n",
    "#llm = OpenAI(temperature=0.0, openai_api_key=\"{{OpenAI_Key}}\")\n",
    "\n",
    "#defining HuggingFace LLM model\n",
    "llm = llm = HuggingFaceEndpoint(repo_id=\"meta-llama/Meta-Llama-3-70B-Instruct\")\n",
    "\n",
    "# creating the prompt template\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"book\"],\n",
    "    template=\"Name the author of the book {book}?\",\n",
    ")\n",
    "\n",
    "# creating the chain\n",
    "chain = LLMChain(llm=llm, \n",
    "                prompt=prompt_template, \n",
    "                verbose=True)\n",
    "\n",
    "# calling the chain\n",
    "print(chain.run(\"The Da Vinci Code\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b7e18d",
   "metadata": {},
   "source": [
    "### Sequential Chains\n",
    "\n",
    "An `LLM chain` can be seen as a basic building block for building a complex system of chains with multiple chains. A `sequential chain` is a sequence of multiple LLM chains where the output of the former acts as the input of the next until we reach the final result.\n",
    "\n",
    "**We can use two types of sequential chains**:\n",
    "\n",
    "1. **Simple sequential chain**: This type of sequential chain only deals with single input and output.\n",
    "\n",
    "2. **Multiple input sequential chain**: This type can handle relatively complex multiple inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691c502e",
   "metadata": {},
   "source": [
    "#### 1. Simple Sequential Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f8d57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the modules\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import SimpleSequentialChain\n",
    "\n",
    "# defining the LLM model for the first chain\n",
    "llm = HuggingFaceEndpoint(repo_id=\"meta-llama/Meta-Llama-3-70B-Instruct\")\n",
    "\n",
    "# creating the prompt template and the first chain\n",
    "prompt_1 = PromptTemplate(\n",
    "    input_variables=[\"book\"],\n",
    "    template=\"Name the author who wrote the book {book}?\"\n",
    ")\n",
    "chain_1 = LLMChain(llm=llm, prompt=prompt_1)\n",
    "\n",
    "# creating the prompt template and the second chain\n",
    "prompt_2 = PromptTemplate(\n",
    "    input_variables=[\"author_name\"],\n",
    "    template=\"Write a 50-word biography for the following author:{author_name}\"\n",
    ")\n",
    "chain_2 = LLMChain(llm=llm, prompt=prompt_2)\n",
    "\n",
    "# combining the chains into a simple sequential chain\n",
    "simple_sequential_chain = SimpleSequentialChain(chains=[chain_1, chain_2],verbose=True)\n",
    "\n",
    "# running the simple sequential chain                                            \n",
    "simple_sequential_chain.run(\"The Da Vinci Code\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2f03ae",
   "metadata": {},
   "source": [
    "The first chain takes the book’s name as an input and returns the author’s name.\n",
    "\n",
    "The second chain then uses the author’s name and returns a 50-word biography of the author."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35a7907",
   "metadata": {},
   "source": [
    "#### 2. Multiple input sequential chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74be666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the modules\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains import SequentialChain\n",
    "\n",
    "# defining the LLM model \n",
    "llm = HuggingFaceEndpoint(repo_id=\"meta-llama/Meta-Llama-3-70B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc706291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the original input \n",
    "biography = \"He is an American author of thriller fiction, best known for his Robert Langdon series. \\\n",
    "          He has sold over 200 million copies of his books, which have been translated into 56 \\\n",
    "          languages. His other works include Angels & Demons, The Lost Symbol, Inferno, and Origin. \\\n",
    "          He is a New York Times best-selling author and has been awarded numerous awards for his \\\n",
    "          writing.\"\n",
    "\n",
    "# creating the prompt template for the first chain\n",
    "prompt_1 = ChatPromptTemplate.from_template(\n",
    "    \"Summarize this biography in one sentence:\"\n",
    "    \"\\n\\n{biography}\"\n",
    ")\n",
    "\n",
    "# we input the original biography, and the output here is the one-line biography\n",
    "chain_1 = LLMChain(llm=llm, prompt=prompt_1, output_key=\"one_line_biography\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0c5719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the prompt template for the second chain\n",
    "prompt_2 = ChatPromptTemplate.from_template(\n",
    "    \"Can you tell the author's name in this biography:\"\n",
    "    \"\\n\\n{one_line_biography}\"\n",
    ")\n",
    "\n",
    "# we input the one-line biography, and the output here is the author's name\n",
    "chain_2 = LLMChain(llm=llm, prompt=prompt_2, output_key=\"author_name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14ba66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the prompt template for the third chain\n",
    "prompt_3 = ChatPromptTemplate.from_template(\n",
    "    \"Tell the name of the highest selling book of this author: \"\n",
    "    \"\\n\\n{author_name}\"\n",
    ")\n",
    "\n",
    "# we input the author's name, and the output here is the highest-selling book\n",
    "chain_3 = LLMChain(llm=llm, prompt=prompt_3, output_key=\"book\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55b6cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the prompt template for the fourth chain\n",
    "prompt_4 = ChatPromptTemplate.from_template(\n",
    "   \"Write a follow-up response to the following \"\n",
    "    \"summary of the highest-selling book of the author:\"\n",
    "    \"\\n\\nAuthor: {author_name}\\n\\nBook: {book}\"\n",
    ")\n",
    "\n",
    "# we input the author's name and the highest-selling book, and the output is the book's summary\n",
    "chain_4 = LLMChain(llm=llm, prompt=prompt_4, output_key=\"summary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd83fd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combining the chains      \n",
    "final_chain = SequentialChain(\n",
    "    chains=[chain_1, chain_2, chain_3, chain_4],\n",
    "    input_variables=[\"biography\"],\n",
    "    output_variables=[\"one_line_biography\", \"author_name\",\"summary\"],\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# calling the final chain\n",
    "print(final_chain(biography))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc98529f",
   "metadata": {},
   "source": [
    "### Memory\n",
    "\n",
    "LangChain provides memory buffers to facilitate a conversational interface where the information exchanged between the user and the model can be stored. These memory buffers act as a context store, allowing the model to retain and utilize relevant information from previous messages in the ongoing conversation.\n",
    "\n",
    "Memory buffers are useful for scenarios such as dialogue systems, chatbots, virtual assistants, and interactive systems that involve multiturn interactions. This enables a more natural and intuitive communication with the language model that is contextually aware.\n",
    "\n",
    "LangChain provides several types of memory to cater to different applications’ needs. Some of those are:\n",
    "\n",
    "1. **Conversation Buffer** : `ConversationBufferMemory` stores the entire message history and hence it can become excessively large sometimes.\n",
    "2. **Conversation Window** : `ConversationBufferWindowMemory` keeps a more manageable list of the last k interactions. This approach ensures the most recent k messages are maintained, preventing the buffer from becoming excessively large.\n",
    "3. **Conversation Summary** : `ConversationSummaryMemory` module stores a summary of the interactions over time instead of keeping all or a limited number of past interactions. This method is advantageous for longer conversations, efficiently storing key information over time without overwhelming the model with excessive storage requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4419a5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing LangChain modules\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "\n",
    "# Insert your key here\n",
    "llm = HuggingFaceEndpoint(repo_id=\"meta-llama/Meta-Llama-3-70B-Instruct\")\n",
    "\n",
    "memory = ConversationBufferWindowMemory(k=2)\n",
    "memory.save_context({\"input\": \"Alex is a 9-year old boy.\"}, \n",
    "                    {\"output\": \"Hello Alex! How can I assist you today?\"})\n",
    "memory.save_context({\"input\": \"Alex likes to play football\"}, \n",
    "                    {\"output\": \"That's great to hear! \"})\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory = memory,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(conversation.predict(input=\"How old is Alex?\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
